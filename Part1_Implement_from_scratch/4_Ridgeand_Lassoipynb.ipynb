{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegressionGD:\n",
        "    def __init__(self, lr=0.01, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float).reshape(-1)\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0.0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            y_pred = X @ self.w + self.b\n",
        "            error = y_pred - y\n",
        "\n",
        "            dw = (2 / n_samples) * (X.T @ error)\n",
        "            db = (2 / n_samples) * np.sum(error)\n",
        "\n",
        "            self.w -= self.lr * dw\n",
        "            self.b -= self.lr * db\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        return X @ self.w + self.b\n",
        "\n",
        "\n",
        "class RidgeRegressionGD(LinearRegressionGD):\n",
        "    def __init__(self, lr=0.01, n_iters=1000, alpha=0.1):\n",
        "        super().__init__(lr, n_iters)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float).reshape(-1)\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0.0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            y_pred = X @ self.w + self.b\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Ridge gradient: add 2*alpha*w (don't penalize bias)\n",
        "            dw = (2 / n_samples) * (X.T @ error) + (2 * self.alpha) * self.w\n",
        "            db = (2 / n_samples) * np.sum(error)\n",
        "\n",
        "            self.w -= self.lr * dw\n",
        "            self.b -= self.lr * db\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "class LassoRegressionGD(LinearRegressionGD):\n",
        "    def __init__(self, lr=0.01, n_iters=1000, alpha=0.1):\n",
        "        super().__init__(lr, n_iters)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float).reshape(-1)\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0.0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            y_pred = X @ self.w + self.b\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Lasso subgradient: add alpha*sign(w) (don't penalize bias)\n",
        "            dw = (2 / n_samples) * (X.T @ error) + self.alpha * np.sign(self.w)\n",
        "            db = (2 / n_samples) * np.sum(error)\n",
        "\n",
        "            self.w -= self.lr * dw\n",
        "            self.b -= self.lr * db\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "# Your data\n",
        "X = [\n",
        "    [1, 2],\n",
        "    [2, 1],\n",
        "    [3, 4],\n",
        "    [4, 3]\n",
        "]\n",
        "y = [8, 9, 18, 19]\n",
        "X_new = [[5, 2]]\n",
        "\n",
        "# Ridge\n",
        "ridge = RidgeRegressionGD(lr=0.01, n_iters=2000, alpha=0.1).fit(X, y)\n",
        "print(\"Ridge Weights:\", ridge.w)\n",
        "print(\"Ridge Bias:\", ridge.b)\n",
        "print(\"Ridge Prediction:\", ridge.predict(X_new))\n",
        "\n",
        "# Lasso\n",
        "lasso = LassoRegressionGD(lr=0.01, n_iters=2000, alpha=0.1).fit(X, y)\n",
        "print(\"Lasso Weights:\", lasso.w)\n",
        "print(\"Lasso Bias:\", lasso.b)\n",
        "print(\"Lasso Prediction:\", lasso.predict(X_new))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpsgrOHQLrN-",
        "outputId": "e477e7fa-dd23-47ff-a522-d1fd900c44cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Weights: [2.79816595 1.96483262]\n",
            "Ridge Bias: 1.5920738718233847\n",
            "Ridge Prediction: [19.51256888]\n",
            "Lasso Weights: [2.97523627 1.97523627]\n",
            "Lasso Bias: 1.12364194075655\n",
            "Lasso Prediction: [19.95029582]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge adds ‚Äúpenalty for large weights‚Äù\n",
        "\n",
        "Just add this to dw:\n",
        "\n",
        "dw=\n",
        "\n",
        "MSE gradient\n",
        "+\n",
        "2\n",
        "ùõº\n",
        "ùë§\n",
        "\n",
        "\n",
        "dw=MSE gradient+2Œ±w\n",
        "Lasso adds ‚Äúpenalty for absolute weights‚Äù\n",
        "\n",
        "Just add:\n",
        "\n",
        "dw=\n",
        "\n",
        "MSE gradient\n",
        "+\n",
        "ùõº\n",
        "‚ãÖ\n",
        "ùë†\n",
        "ùëñ\n",
        "ùëî\n",
        "ùëõ\n",
        "(\n",
        "ùë§\n",
        ")\n",
        "dw=MSE gradient+Œ±‚ãÖsign(w)\n",
        "\n",
        "Bias b stays unchanged."
      ],
      "metadata": {
        "id": "i8BpAffgNshy"
      }
    }
  ]
}